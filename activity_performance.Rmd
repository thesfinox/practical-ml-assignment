---
title: "Sport Wearable Devices and Activity Performance"
author: "Riccardo Finotello"
date: "24 June 2020"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

In this analysis we will consider data of sport wearable devices and we will
tackle the task of predicting the performance of dumbell exercises as perceived
from the device. In other words we will try to predict the assigned class of the
exercise from motion sensor data.

## Cleaning the Data

We will first access the training data:

```{r cache = TRUE}
data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 na.strings = c("#DIV/0!", "", "NA"),
                 stringsAsFactors = FALSE
                )
```

We then take a look at the number of entries and features in the dataset:

```{r}
print(paste("No. of entries:", nrow(data)))
print(paste("No. of columns<:", ncol(data)))
```

From the dataset we remove information related to the specific user and keep
only numerical features and non empty columns:

```{r cache=TRUE}
data <- data[,-(1:7)]
data$classe <- as.factor(data$classe) # convert classe to factor
```

We then compute the fraction of **NA** data in each column and remove those
variables with more than 50% of missing data. This shows that we will keep only
a fraction of the original variables:

```{r cache=TRUE}
# find fraction of NA values
data.na.fraction <- sapply(data, function(x) {mean(is.na(x))})
data.cols <- (data.na.fraction <= 0.5)
print(paste("Fraction of retained variables:", round(mean(data.cols),2)))

# select the relevant columns
train.data <- data[, data.cols]
```
We therefore have a tidy dataset containing
`r 100 * mean(complete.cases(train.data))`% of complete cases and new dimensions:

```{r}
print(paste("No. of entries:", nrow(train.data)))
print(paste("No. of columns:", ncol(train.data)))
```

We finally divide the features we use for training from the labels we try to
predict:

```{r}
# shuffle the dataset
train.data <- train.data[sample(nrow(train.data)),]

# store the id of the classes (last column)
labels <- c(ncol(train.data))
```

As an additional step before the exploratory data analysis, we divide the
training set into a further partition for testing:

```{r message=FALSE}
library(caret)
train.part <- createDataPartition(train.data[,ncol(train.data)], p=0.8, list=FALSE)
train <- train.data[train.part,]
test <- train.data[-train.part,]
```

## Exploratory Data Analysis

To better understand the distribution of the variables, we study their
correlations properties (**we consider only the training partition and we do not
look at the labels to avoid biasing the strategy**):

```{r message=FALSE}
library(reshape2)
corr.mat <- cor(train[,-labels])
corr.mat.melt <- melt(corr.mat)

# plot the correlation matrix
library(ggplot2)
g <- ggplot(data = corr.mat.melt, aes(Var1, Var2, fill = value)) +
     geom_tile() +
     xlab("") +
     ylab("") +
     scale_fill_gradient2(low = "red", mid= "white", high = "blue",
                          midpoint = 0, limit = c(-1,1)
                         ) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
print(g)
```

As we can see the variables are in general not correlated, apart from (mainly)
the accelerometers and gyroscope (partly as expected). We may expect them to
play a greater part in the prediction or at least we will see that they will be
more interacting.

In fact we can dig a bit more on the subject and visualise the distribution of
these sensors with respect to the `classe` label:

```{r message=FALSE}
library(gridExtra)
g1 <- ggplot(data = train,
             aes(x = accel_belt_x, y = accel_belt_y, col = accel_belt_z)) +
      geom_point() +
      scale_color_gradient2(low = "red", mid = "white", high = "blue",
                            midpoint = 0)
g2 <- ggplot(data = train,
             aes(x = gyros_belt_x, y = gyros_belt_y, col = gyros_belt_z)) +
      geom_point() +
      scale_color_gradient2(low = "red", mid = "white", high = "blue",
                            midpoint = 0)

grid.arrange(g1, g2, nrow = 1)
```

We then divide the training variables from the labels:

```{r}
x.train <- train[,-labels]
y.train <- train[,labels]
x.test  <- test[,-labels]
y.test  <- test[,labels]
```

The summary of the training variables can then be useful to understand the next
step in the analysis:

```{r}
library(skimr)
skim(x.train)
```

## Machine Learning Predictions

we first preprocess the data and standardise the training features:

```{r}
preprocess <- preProcess(x.train, method = c("center", "scale"))
x.new.train <- predict(preprocess, newdata = x.train)
x.new.test  <- predict(preprocess, newdata = x.test)
```

We finally apply a **random forest** algorithm to the training data. We use a
10-fold cross-validation asstrategy (partly to prevent overfit and improve
predictions):

```{r cache=TRUE, message=FALSE}
# set multithreading
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# begin training
set.seed(42)
train.ctrl <- trainControl(method = "cv", number = 10, search = "grid")
train.rf <- train(x.new.train, y.train, trControl = train.ctrl, method = "rf")

# stop multithreading
stopCluster(cl)
```

We the show the summary of the training procedure:
```{r}
print(train.rf)
```

In training we performed a hyperparameter grid search in order to improve the
accuracy of the prediction. In fact we show the plot of such search:

```{r}
plot(train.rf, main="Accuracy over the Hyperparameter Search")
```

We finally plot the feature importance assigned by the algorithm to check that
our analysis (including the exploratory analysis and dataset cleaning) was in
fact meaningful:

```{r}
var.imp <- varImp(train.rf)
plot(var.imp, main = "Variable Ranking (random forest)")
```

Finally we show the training performance:

```{r}
train.pred <- predict(train.rf, newdata = x.new.train)
confusionMatrix(data = train.pred, reference = y.train, mode = "everything")
```

and for the test data:

```{r}
test.pred <- predict(train.rf, newdata = x.new.test)
confusionMatrix(data = test.pred, reference = y.test, mode = "everything")
```

We finally plot the distribution of the **validation set** and compare the
predictions:

```{r cache = TRUE, fig.cap = "\\label{fig:pred}Predictions on the validation set: coloured bars represent true values and white superimposed values represent predictions."}
library(dplyr)
library(data.table)
test.pred <- data.table(test.pred)
test.pred.n <- test.pred %>% count(test.pred)
y.test <- data.table(y.test)
y.test.n <- y.test %>% count(y.test)

# plot the counts
g <- ggplot() +
     geom_bar(data = test.pred , aes(test.pred, fill = test.pred), width = 0.75) +
     geom_bar(data = y.test, aes(y.test), width = 0.3, fill = "white") +
     xlab("classe") +
     ylab("count") +
     ggtitle("Validation set predictions and true values")
print(g)
```

## Test Set Predictions

We finally consider the final test set:

```{r cache = TRUE}
final.test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                 na.strings = c("#DIV/0!", "", "NA"),
                 stringsAsFactors = FALSE)

# perform the same transformations
final.test <- final.test[,-(1:7)]
final.test$problem_id <- as.factor(final.test$problem_id)
test.data.na.fraction <- sapply(final.test, function(x) {mean(is.na(x))})
test.data.cols <- (test.data.na.fraction <= 0.5)

# select the relevant columns
test.data <- final.test[, test.data.cols]
test.data <- test.data[,-ncol(test.data)]

# apply pre-process transformation
test.data <- predict(preprocess, newdata = test.data)
```

We can now make the test set predictions:

```{r}
final.predictions <- predict(train.rf, newdata = test.data)
print(final.predictions)
```

## Conclusions

We showed that we were able to make meaningful predictions on a cleaned version
of the wearable database, achieving a very high level of accuracy (more than 99%
of accuracy has been reached) using a random forest approach.
